{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7394a8cd",
   "metadata": {},
   "source": [
    "# LLM Clients\n",
    "\n",
    "You can consult the following trustable and updated LLM providers performance leaderboards to monitor latest performance metrics and cost per tokens:\n",
    "\n",
    "- [Hugging Face LLM Performance Leaderboard](https://huggingface.co/spaces/ArtificialAnalysis/LLM-Performance-Leaderboard)\n",
    "- [Artificial Analysis LLM API Providers Leaderboard](https://artificialanalysis.ai/leaderboards/providers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcf75f3",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3544ebc4",
   "metadata": {},
   "source": [
    "Import required Python packages and get application settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec02ba06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General purpose libraries\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc3b042",
   "metadata": {},
   "source": [
    "## Google Gemini\n",
    "\n",
    "To use the Google Gen AI client directly or with Langchain, the following environment variable has to be set.\n",
    "\n",
    "```\n",
    "GOOGLE_API_KEY=\"your-api-key\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c4606a",
   "metadata": {},
   "source": [
    "### Google Gemini Genarative Language API\n",
    "\n",
    "The Gemini API is consumed through the Google `Gen AI / Gemini API` named as `Generative Language` API in GCP.\n",
    "\n",
    "Relevant Information Sources:\n",
    "\n",
    "-[Google Gemini API Official Docs - All available model IDs information section](https://ai.google.dev/gemini-api/docs/models)\n",
    "\n",
    "-[Google Gemini / GenAI Github Repository](https://github.com/googleapis/python-genai?tab=readme-ov-file#google-gen-ai-sdk)\n",
    "\n",
    "-[API Key Reference and Best Place for Creation and Monitoring](https://aistudio.google.com/usage)\n",
    "\n",
    "-[Model Comparison and Pricing for 2.0 Generation](https://developers.googleblog.com/en/gemini-2-family-expands/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37667b1d",
   "metadata": {},
   "source": [
    "For getting the information of all the avialble models in the Gemini API, we can consult this [reference](https://ai.google.dev/gemini-api/docs/models).\n",
    "\n",
    "To list all the available modesl through code, we can use the following code block taken from this official [example](https://ai.google.dev/api/models).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8acf1389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='models/gemini-2.0-flash-lite' display_name='Gemini 2.0 Flash-Lite' description='Gemini 2.0 Flash-Lite' version='2.0' endpoints=None labels=None tuned_model_info=TunedModelInfo(base_model=None, create_time=None, update_time=None) input_token_limit=1048576 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "# Create Google GenAI client\n",
    "client = genai.Client()\n",
    "\n",
    "# Get model information for a specific model\n",
    "model_info = client.models.get(model=\"gemini-2.0-flash-lite\")\n",
    "print(model_info)\n",
    "\n",
    "# # List all LLM-based model IDs available in the GenAI API\n",
    "# print(\"List of models that support generateContent:\\n\")\n",
    "# for m in client.models.list():\n",
    "#     for action in m.supported_actions:\n",
    "#         if action == \"generateContent\":\n",
    "#             print(m.name)\n",
    "\n",
    "# # Embedding model IDs can also be fetched through the client\n",
    "# print(\"\\nList of models that support embedContent:\\n\")\n",
    "# for m in client.models.list():\n",
    "#     for action in m.supported_actions:\n",
    "#         if action == \"embedContent\":\n",
    "#             print(m.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e48c822",
   "metadata": {},
   "source": [
    "### Langchain Google Generative AI Chat model\n",
    "\n",
    "The `ChatGoogleGenerativeAI` chat model wraps the Google Gen AI API services to be consumed by Langchain.\n",
    "\n",
    "A small demo is provided next, but all the _know-hows_ can be easily found in the official [`ChatGoogleGenerativeAI` docs](https://python.langchain.com/docs/integrations/chat/google_generative_ai/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771563c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Create a Lanchain Chat Model for Gemini\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash-lite\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47edd4bd",
   "metadata": {},
   "source": [
    "Invocation demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ed5d5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"J'adore la programmation.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash-lite', 'safety_ratings': []}, id='run--a5dd906a-ada1-4ac6-b308-8a20e8272972-0', usage_metadata={'input_tokens': 20, 'output_tokens': 7, 'total_tokens': 27, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the LLM model inference\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "ai_msg = llm.invoke(messages)\n",
    "ai_msg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
